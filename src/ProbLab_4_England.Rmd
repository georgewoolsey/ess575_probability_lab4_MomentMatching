---
title: "ESS 575: Probability Lab 4 - Moment Matching"
author: "Team England" 
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
header-includes:
  - \usepackage{caption}
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding){ 
    out_dir <- '../';
    rmarkdown::render(inputFile, encoding = encoding, output_file=file.path(dirname(inputFile), out_dir, 'ProbLab_4_England.pdf')) 
  })
---

Team England:

  - Caroline Blommel
  - Carolyn Coyle
  - Bryn Crosby
  - George Woolsey
  
cblommel@mail.colostate.edu, carolynm@mail.colostate.edu, brcrosby@rams.colostate.edu, george.woolsey@colostate.edu


```{r setup, include=FALSE}
# load packages
library(tidyverse)
library(lubridate)
library(viridis)
library(scales)
library(kableExtra)
library(ggpubr)
library(latex2exp)
# knit options
knitr::opts_chunk$set(
  echo = TRUE
  , warning = FALSE
  , message = FALSE
  , fig.height = 5
  , fig.width = 7
)
# set seed
set.seed(10)
```

When we say *support*, we are referring to the values of a random variable for which probability density or probability exceed 0 and are defined. The support of lognormal distribution is continuous and strictly non-negative, which makes it particularly useful in ecology. Moreover, it is often useful because it is asymmetric, allowing for values that are extreme in the positive direction. Finally, it is useful for representing products of random variables. The central limit theorem would predict that the distribution of sums of random variables will be normal, no matter how each is individually distributed. The products of random variables will be lognormally distributed regardless of their individual distributions.

If a random variable is lognormally distributed then the log of that random variable is normally distributed (conversely, if you exponentiate a normal random variable it generates a lognormal random variable). The first parameter of the lognormal distribution is the mean of the random variable on the log scale (i.e., $\alpha$ on cheat sheet, `meanlog` in R) and the second parameter is the variance (or sometimes the standard deviation) of the random variable on the log scale (i.e., $\beta$ on cheatsheet, `sdlog` in R). We often predict the median of the distribution with our deterministic model, which means that we can use the log of the median as the first parameter because

\begin{eqnarray}
z \sim  \sf{lognormal}(\alpha,\beta)\\
\sf{median}(z) = e^{\alpha}\\
\sf{log}(\sf{median}(z)) = \alpha
\end{eqnarray}

# Question 1

Simulate 10,000 data points from a normal distribution with mean 0 and standard deviation 1 and another 10,000 data points from a log normal distribution with first parameter (the mean of the random variable on the log scale) = 0 and second parameter (the standard deviation of the parameter on the log scale) = 1. Display side-by-side histograms scaled to the density. Find the mean and variance of the lognormal distribution using moment matching. Check your moment-matched values empirically with the simulated data. The moment-matched values and the empirical values are close for the mean, but less so for the variance. Why? What happens when you increase the number or draws? Explore the two distributions by repeating with different means and standard deviations of your choice.


```{r}
mean <- 0
sd <- 1
n <- 100000
# normal
dist_norm <- rnorm(n = n, mean = mean, sd = sd)

# log-normal
dist_lognorm <- rlnorm(n = n, meanlog = mean, sdlog = sd)

# plot historgram norm
  p_norm <- 
    ggplot(data = data.frame(dist_norm), aes(x = dist_norm)) +
    geom_histogram(
      aes(y = ..density..)
      , fill = "steelblue1"
      , alpha = 0.7
      , color = "black"
      , lwd = 0.5
    ) +
    geom_density(
      linetype = 2
      , lwd = 1.2
      , color = "orangered"
    ) +
    xlab("y") +
    ylab("Density") +
    labs(
      title = "Normal Distribution"
    ) +
    theme_bw()

# plot historgram log-norm
  p_lognorm <- 
    ggplot(data = data.frame(dist_lognorm), aes(x = dist_lognorm)) +
    geom_histogram(
      aes(y = ..density..)
      , fill = "steelblue4"
      , alpha = 0.7
      , color = "black"
      , lwd = 0.5
    ) +
    geom_density(
      linetype = 2
      , lwd = 1.2
      , color = "orangered"
    ) +
    xlab("log(y)") +
    ylab("Density") +
    labs(
      title = "Lognormal Distribution"
    ) +
    theme_bw()

# combine charts
  ggpubr::ggarrange(
      p_norm, p_lognorm
      , nrow = 1
      , ncol = 2
    )

# empirically simulate data
mean_lognorm <- exp(mean + sd^2/2)
var_lognorm <- (exp(sd^2) - 1) * exp(2*mean + sd^2)
```

Find the mean and variance of the lognormal distribution using moment matching. Check your moment-matched values empirically with the simulated data. 

\textcolor{blue}{The mean of the lognormal distribution is: }`r round(mean(dist_lognorm), 3)`

\textcolor{blue}{The variance of the lognormal distribution is: }`r round(var(dist_lognorm), 3)`

\textcolor{blue}{The empirical mean of the lognormal distribution ($\mu = e^{\alpha+\frac{\beta^2}{2}}$) is: }`r round(mean_lognorm, 3)`

\textcolor{blue}{The empirical variance of the lognormal distribution ($\sigma^2 = (e^{\beta^{2}}-1) \cdot e^{2\alpha+\beta^{2}}$) is: }`r round(var_lognorm, 3)`

The moment-matched values and the empirical values are close for the mean, but less so for the variance. Why? What happens when you increase the number or draws? Explore the two distributions by repeating with different means and standard deviations of your choice.

\textcolor{blue}{The moment-matched values and the empirical values for the mean and variance are close but less so for the variance than the mean. This occurs because the variance of the lognormal distribution is a product of two exponential functions. When we simulate the data using $\sf{rlnorm}$ in R, increasing the number of simulated values results in the moment-matched values approaching the emprical values as $n$ approaches $+\infty$. }

# Question 2 - Question 8

# Question 9

You are modeling the relationship between plant growth rate and soil water. Represent plant growth ($\mu_i$) as a linear function of soil water, $\mu_i = \beta_0 + \beta_1 x_i$. Write out the model for the data. Simulate a data set of 20, strictly non-negative pairs of $y$ and $x$ values. Plot the data and overlay the generating model. Assume that:

* Soil water, the $x$ value, varies randomly and uniformly between 0.01 and 0.2
* $\beta_0 = 0.01$ and $\beta_1 = 0.9$
* the standard deviation of the model prediction is $\sigma=0.03$

```{r}
n <- 20
x <- runif(n = n, min = 0.01, max = 0.2)
b0 <- 0.01
b1 <- 0.9
sd <- 0.03
# calculate plant growth mu
mu <- b0 + b1 * x
# calculate the shape alpha for gamma dist
alpha <- (mu^2)/(sd^2)
# calculate the rate beta for gamma dist
beta <- (mu)/(sd^2)
# simulate y of strictly non-negative values using the gamma dist.
y <- rgamma(n = n, shape = alpha, rate = beta)
# data
dta <- data.frame(
  x = x
  , mu = mu
  , y = y
)
# plot
ggplot(data = dta) +
  geom_point(
    aes(x = x, y = y)
    , alpha = 0.7
    , size = 3
    , color = "gray35"
  ) +
  geom_line(
    aes(x = x, y = mu)
    , linetype = 2
    , lwd = 1.2
    , alpha = 0.7
    , color = "black"
  ) +
  xlab("Soil Water") +
  ylab("Plant Growth") +
  labs(
    title = "Relationship between plant growth rate and soil water"
  ) +
  theme_bw()

```



# Question 10

The negative binomial distribution is a more robust alternative to the Poisson distribution, allowing the variance to differ from the mean. There are two parameterizations for the negative binomial. 

The first parameterization of the negative binomial distribution is more frequently used by ecologists: 

$$
[z\mid\lambda,r] = 
\cfrac{\Gamma (z + r)}{\Gamma (r)z!}\Big(\cfrac{r}{r+\lambda}\Big)^{r}
\Big(\cfrac{\lambda}{r+\lambda}\Big)^{z}\textrm{,}
$$

where $z$ is a discrete random variable, $\lambda$ is the mean of the distribution, and $r$ is the *dispersion parameter*, also called the size. The variance of $z$ is:

$$
\sigma^{2}=\lambda + \cfrac{\lambda^{2}}{r}
$$

The second parameterization is more often implemented in coding environments (i.e. JAGS):

$$
[z \mid r,\phi] = \cfrac{\Gamma(z+r)}{\Gamma(r)z!}\phi^{r}(1-\phi)^{z}\textrm{,}
$$

where $z$ is the discrete random variable representing the number of failures that occur in a sequence of Bernoulli trials before $r$ successes are obtained. The parameter $\phi$ is the probability of success on a given trial. Where $\phi$, the probability of success on a given trial is:

$$
\phi=\cfrac{r}{\big(\lambda+r\big)}
$$
Use the `rnbinom` function in R to simulate 100,000 observations from a negative binomial distribution with mean of $\mu$=100 and variance of $\sigma^2$=400 using the **first** parameterization that has a mean and a dispersion parameter. (Hint: find an expression for $r$ and moment match.) Do the same simulation using the **second** parameterization. Plot side-by-side histograms of the simulated data.

```{r}
n <- 100000
mean <- 100
var <- 400
# the dispersion param
r <- mean^2/(var - mean)
# first parameterization
dist_negbinom1 <- rnbinom(n = n, mu = mean, size = r)

# phi = probability of success in given trial
phi <- r/(mean+r)
# second parameterization
dist_negbinom2 <- rnbinom(n, prob = phi ,size = r)

```

\textcolor{blue}{The mean of the first parameterization negative binomial distribution as simulated is: }`r round(mean(dist_negbinom1), 3)`

\textcolor{blue}{The variance of the first parameterization negative binomial distribution as simulated is: }`r round(var(dist_negbinom1), 3)`

\textcolor{blue}{The mean of the second parameterization negative binomial distribution as simulated is: }`r round(mean(dist_negbinom2), 3)`

\textcolor{blue}{The variance of the second parameterization negative binomial distribution as simulated is: }`r round(var(dist_negbinom2), 3)`

Plot side-by-side histograms of the simulated data.

```{r}

# plot historgram param 1
  p_dist_negbinom1 <- 
    ggplot(data = data.frame(dist_negbinom1), aes(x = dist_negbinom1)) +
    geom_histogram(
      fill = "darkorange1"
      , alpha = 0.7
      , color = "black"
      , lwd = 0.4
    ) +
    xlab(latex2exp::TeX("$z$")) +
    ylab("Count") +
    labs(
      title = "Negative binomial distribution"
      , subtitle = "First parameterization"
    ) +
    theme_bw()
# plot historgram param 2
  p_dist_negbinom2 <- 
    ggplot(data = data.frame(dist_negbinom2), aes(x = dist_negbinom2)) +
    geom_histogram(
      fill = "darkorange4"
      , alpha = 0.7
      , color = "black"
      , lwd = 0.4
    ) +
    xlab(latex2exp::TeX("$z$")) +
    ylab("Count") +
    labs(
      title = "Negative binomial distribution"
      , subtitle = "Second parameterization"
    ) +
    theme_bw()

# combine charts
  ggpubr::ggarrange(
      p_dist_negbinom1, p_dist_negbinom2
      , nrow = 1
      , ncol = 2
    )
```


